AUTOCREW_VERSION = "2.0.5"

# Please do not edit this file directly
# Please modify the config file, "config.ini"
# If you experience any errors, please upload the complete log file, "autocrew.log", along with your issue on GitHub:
# https://github.com/yanniedog/autocrew/issues/new 



import argparse
import configparser
import copy
import csv
import io
import json
import logging
import os
import re
import requests
import shutil
import subprocess
import sys
import tiktoken
import time
from datetime import datetime
from packaging import version
from typing import Any, Dict, List

from crewai import Agent, Crew, Process, Task
from langchain_community.llms import Ollama
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from openai import OpenAI



GREEK_ALPHABETS = ["alpha", "beta", "gamma", "delta", "epsilon", "zeta", "eta", "theta", "iota", "kappa",
                       "lambda", "mu", "nu", "xi", "omicron", "pi", "rho", "sigma", "tau", "upsilon"]



def install_dependencies():
    # Check if 'requirements.txt' exists in the current working directory
    requirements_file = 'requirements.txt'
    if not os.path.exists(requirements_file):
        raise FileNotFoundError(f"{requirements_file} not found in the current working directory.")

    # Check if 'pip' is available
    pip_executable = shutil.which('pip') or shutil.which('pip3')
    if not pip_executable:
        raise EnvironmentError("pip is not available on the system.")

    logging.info("Installing dependencies...")

    # Run the pip install command and capture the output
    result = subprocess.run([pip_executable, 'install', '-r', requirements_file], capture_output=True, text=True)

    # Check if the installation was successful
    if result.returncode != 0:
        # If there was an error, print the error message and raise an exception
        logging.error("Error occurred while installing dependencies:")
        logging.error(result.stdout)
        logging.error(result.stderr)

        raise RuntimeError("Failed to install dependencies.")
    else:
        print("Dependencies installed successfully.")

    
def initialize_logging(verbose=False, on_screen_logging_level='INFO', message=None):
    log_file = os.path.join(os.getcwd(), 'autocrew.log')

    # Set up the logger to write to the log file
    file_handler = logging.FileHandler(log_file, mode='w')
    file_handler.setLevel(logging.DEBUG)  # Always set to DEBUG for the log file
    # Include function name and line number in the specified format
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - (function: %(funcName)s, line: %(lineno)d) - %(message)s'))

    # Set up the logger to write to the console
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.DEBUG if verbose else getattr(logging, on_screen_logging_level.upper(), logging.INFO))
    # Include function name and line number in the specified format
    console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - (function: %(funcName)s, line: %(lineno)d) - %(message)s'))

    # Get the root logger and add the handlers to it
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)  # Set the root logger level to DEBUG to capture all logs
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    # Ensure that all other loggers use the root logger's configuration
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    logging.root.addHandler(file_handler)
    logging.root.addHandler(console_handler)

    # Log the initial message at the top of the log file if provided
    if message:
        logger.info(message)

    # Log the script version number
    logger.info(f"AutoCrew version: {AUTOCREW_VERSION}")








class AutoCrew:
    def __init__(self, config_file='config.ini'):
        self.config = configparser.ConfigParser()
        if not os.path.exists(config_file):
            raise FileNotFoundError(f"Config file {config_file} not found.")
        self.config.read(config_file)

        # BASIC section
        self.llm_endpoint = self.config.get('BASIC', 'llm_endpoint', fallback=None)

        # OLLAMA_CONFIG section
        self.llm_model = self.config.get('OLLAMA_CONFIG', 'llm_model', fallback=None)

        # OPENAI_CONFIG section
        self.openai_model = self.config.get('OPENAI_CONFIG', 'openai_model', fallback=None)
        try:
            self.openai_max_tokens = int(self.config.get('OPENAI_CONFIG', 'max_tokens', fallback=0))
            logging.debug(f"Loaded max_tokens from config: {self.openai_max_tokens}")
        except ValueError:
            logging.error("Invalid value for max_tokens in config file.")
            self.openai_max_tokens = 0

        # CREWAI_SCRIPTS section
        self.llm_endpoint_within_generated_scripts = self.config.get('CREWAI_SCRIPTS', 'llm_endpoint_within_generated_scripts', fallback=None)
        self.llm_model_within_generated_scripts = self.config.get('CREWAI_SCRIPTS', 'llm_model_within_generated_scripts', fallback=None)
        self.add_api_keys_to_crewai_scripts = self.config.getboolean('CREWAI_SCRIPTS', 'add_api_keys_to_crewai_scripts', fallback=False)
        self.add_ollama_host_url_to_crewai_scripts = self.config.getboolean('CREWAI_SCRIPTS', 'add_ollama_host_url_to_crewai_scripts', fallback=False)

        # AUTHENTICATORS section
        self.openai_api_key = self.config.get('AUTHENTICATORS', 'openai_api_key', fallback=None)
        self.ngrok_auth_token = self.config.get('AUTHENTICATORS', 'ngrok_auth_token', fallback=None)
        self.ngrok_api_key = self.config.get('AUTHENTICATORS', 'ngrok_api_key', fallback=None)

        # REMOTE_HOST_CONFIG section
        self.reset_ollama_host_on_startup = self.config.getboolean('REMOTE_HOST_CONFIG', 'reset_ollama_host_on_startup', fallback=False)
        self.use_remote_ollama_host = self.config.getboolean('REMOTE_HOST_CONFIG', 'use_remote_ollama_host', fallback=False)
        self.name_of_remote_ollama_host = self.config.get('REMOTE_HOST_CONFIG', 'name_of_remote_ollama_host', fallback=None)

        # MISCELLANEOUS section
        self.on_screen_logging_level = self.config.get('MISCELLANEOUS', 'on_screen_logging_level', fallback='INFO')

        # Set a default value for ollama_host
        self.ollama_host = "http://localhost:11434"  # Default value

        # Initialize other components
        self.ollama = self.initialize_ollama() if self.llm_endpoint == 'ollama' else None
        # self.openai = self.initialize_openai() if self.llm_endpoint == 'openai' else None  (this is not needed, as openai does not need to be initalised like Ollama)

        
    def countdown_timer(seconds: int):
        """Displays a countdown timer for the specified number of seconds."""
        for i in range(seconds, 0, -1):
            logging.info(f"Pausing for {i} seconds")
            time.sleep(1)
        logging.info("Continuing...")

    def initialize_ollama(self):
        connection_type = "remote" if self.use_remote_ollama_host else "local"
        model = self.llm_model
        logging.info(f"Initializing {connection_type} connection to Ollama using model {model}...")

        # Start the Ollama service if it's not already running
        self.start_ollama_service()

        # Set default Ollama host if not specified in environment
        self.ollama_host = os.getenv('OLLAMA_HOST', 'http://localhost:11434')
        logging.info(f"Ollama host: {self.ollama_host}")

        try:
            return Ollama(base_url=self.ollama_host, model=self.llm_model, verbose=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))
        except Exception as e:
            logging.error(f"Failed to initialize Ollama: {e}")
            return None

    def start_ollama_service(self):
        try:
            # Check if the Ollama service is running
            subprocess.check_output(["pgrep", "-f", "ollama serve"])
            logging.info("Ollama service is already running.")
        except subprocess.CalledProcessError:
            # If the Ollama service is not running, start it
            logging.info("Starting Ollama service...")
            subprocess.Popen(["ollama", "serve"], start_new_session=True)



    @staticmethod
    def count_tokens(string: str) -> int:
        """Returns the number of tokens in a text string."""
        encoding_name = 'cl100k_base'  # Assuming this is the encoding you want to use
        encoding = tiktoken.get_encoding(encoding_name)
        num_tokens = len(encoding.encode(string))
        return num_tokens
    


    def get_agent_data(self, overall_goal, delimiter):
        if self.llm_endpoint == 'ollama':
            connection_type = "remote" if self.use_remote_ollama_host else "local"
            model = self.llm_model
            llm_name = f"Ollama using model {model}"
        else:
            connection_type = "remote"
            model = self.openai_model
            llm_name = f"OpenAI using model {model}"

        logging.info(f"Initializing {connection_type} connection to {llm_name} for generating agent data with the overall goal of '{overall_goal}'...")
        
        # Construct the instruction including the overall_goal
        instruction = (
            f'Create a dataset in a CSV format with each field enclosed in double quotes, '
            f'for a team of agents with the goal: "{overall_goal}". '
            f'Use the delimiter "{delimiter}" to separate the fields. '
            'Include columns "role", "goal", "backstory", "assigned_task", "allow_delegation". '
            'Each agent\'s details should be in quotes to avoid confusion with the delimiter. '
            'Provide a single-word role, individual goal, brief backstory, assigned task, and delegation ability (True/False) for each agent.'
        )
        
        # Calculate the number of tokens in the complete instruction
        instruction_tokens = self.count_tokens(instruction)
        logging.info(f"Instruction given to {llm_name}:\n{instruction}")
        logging.info(f"Number of tokens in the instruction: {instruction_tokens}")

        # Read the max_tokens parameter from config.ini
        max_tokens = self.openai_max_tokens
        logging.info(f"Max tokens from config: {max_tokens}")

        # Subtract the number of tokens in the instruction from max_tokens
        max_response_tokens = max_tokens - instruction_tokens - 200
        logging.info(f"Max response tokens available for LLM response: {max_response_tokens}")

        # Ensure max_response_tokens is not negative
        if max_response_tokens < 0:
            logging.error("The number of tokens in the instruction exceeds the max_tokens limit.")
            max_response_tokens = 0

        try:
            if self.llm_endpoint == 'ollama' and self.ollama:
                response = self.ollama.invoke(instruction)
                # Log the raw LLM output
                logging.info(f"Raw LLM output (Ollama):\n{response}")
                logging.info(f"Number of tokens in the response: {self.count_tokens(response)}")
                return response
            elif self.llm_endpoint == 'openai' and self.openai_api_key:
                client = OpenAI(api_key=self.openai_api_key)
                chat_completion = client.chat.completions.create(
                    model=self.openai_model,  # Use the model directly from the configuration
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant."},
                        {"role": "user", "content": instruction}
                    ],
                    max_tokens=max_response_tokens  # Use the calculated max_response_tokens
                )
                response = chat_completion.choices[0].message.content.strip()
                # Log the raw LLM output
                logging.info(f"Raw LLM output (OpenAI):\n{response}")
                logging.info(f"Number of tokens in the response: {self.count_tokens(response)}")
                return response
            else:
                logging.error("Neither OpenAI API key nor Ollama instance is available.")
                return ""
        except Exception as e:
            logging.error(f"Error in API call: {e}")
            return ""




    def get_next_crew_name(self, overall_goal):
        # Define the target directory as 'generated' within the current working directory
        target_directory = os.path.join(os.getcwd(), "scripts")
        
        # Check if the target directory exists, create it if not
        if not os.path.exists(target_directory):
            os.makedirs(target_directory)

        formatted_goal = overall_goal.replace(" ", "-")
        existing_files = [f for f in os.listdir(target_directory) if (f.endswith('.csv') or f.endswith('.py')) and formatted_goal in f]
        existing_indices = []

        for file_name in existing_files:
            name_parts = file_name.split('-')
            if len(name_parts) > 3:  # Ensure the filename has enough parts
                alpha = name_parts[-1].split('.')[0]  # Get the part before file extension
                if alpha in GREEK_ALPHABETS:
                    existing_indices.append(GREEK_ALPHABETS.index(alpha))

        if existing_indices:
            next_index = (max(existing_indices) + 1) % len(GREEK_ALPHABETS)
        else:
            next_index = 0

        return GREEK_ALPHABETS[next_index]





    def save_csv_output(self, response, overall_goal):
        # Create a CSV reader to parse the response
        reader = csv.reader(io.StringIO(response), quotechar='"', delimiter=',', skipinitialspace=True)
        
        cleaned_csv_lines = []
        for fields in reader:
            # Skip lines that don't have exactly 4 commas (5 fields)
            if len(fields) != 5:
                continue
            # Clean each field and enclose in double quotes
            cleaned_fields = ['"{}"'.format(field.replace('"', '""')) for field in fields]
            cleaned_line = ','.join(cleaned_fields)
            cleaned_csv_lines.append(cleaned_line)

        if cleaned_csv_lines:
            csv_data = '\n'.join(cleaned_csv_lines)
            logging.info("Extracted and cleaned CSV data from raw output.")
            logging.info(f"Final CSV content before saving:\n{csv_data}")
        else:
            logging.error("No CSV data found in the response.")
            raise ValueError("No CSV data found in the response")

        # Save the cleaned CSV data to a file
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        greek_suffix = self.get_next_crew_name(overall_goal)  # Get the next Greek alphabet suffix
        file_name = f'crewai-autocrew-{timestamp}-{overall_goal.replace(" ", "-")}-{greek_suffix}.csv'
        directory = os.path.join(os.getcwd(), "scripts")
        if not os.path.exists(directory):
            os.makedirs(directory)
        file_path = os.path.join(directory, file_name)
        
        with open(file_path, 'w') as file:
            file.write(f'# {file_path}\n')  # Add the file path as a comment at the top of the CSV file
            file.write(csv_data)
            logging.info(f"CSV file saved at: {file_path}")

        return file_path




    def parse_csv_data(self, response, delimiter=',', filename=''):
        # Extract only the CSV content using regex
        csv_pattern = r'("role","goal","backstory","assigned_task","allow_delegation".*?)(?:```|$)'
        match = re.search(csv_pattern, response, re.DOTALL)
        if not match:
            logging.error("CSV data not found in the response.")
            raise ValueError('CSV data not found in the response')

        csv_data = match.group(1).strip()  # Remove any extra whitespace
        logging.info(f"Extracted CSV data for parsing:\n{csv_data}")

        header = ['role', 'goal', 'backstory', 'assigned_task', 'allow_delegation']
        agents_data = []

        try:
            csv_reader = csv.reader(io.StringIO(csv_data), delimiter=delimiter)
            lines = list(csv_reader)
        except Exception as e:
            logging.error(f"Error reading CSV data: {e}")
            raise

        if not lines:
            logging.error("CSV data is empty after splitting into lines.")
            raise ValueError('CSV data is empty')

        header_line = lines[0]
        header_indices = {h.lower(): i for i, h in enumerate(header_line)}

        for required_header in header:
            if required_header not in header_indices:
                logging.error(f'Missing required header "{required_header}" in CSV data')
                raise ValueError(f'Missing required header "{required_header}"')

        for line in lines[1:]:  # Skip the header line
            agent_data = {}
            for header_name in header:
                header_index = header_indices.get(header_name.lower())
                agent_data[header_name] = line[header_index].strip('"').strip() if header_index is not None and header_index < len(line) else None
            if 'role' not in agent_data or not agent_data['role']:
                logging.error('Role component missing in line of CSV data')
                raise ValueError('Role component missing in CSV data')
            agent_data['filename'] = filename
            agents_data.append(agent_data)

        logging.info(f"Successfully parsed {len(agents_data)} agents from CSV data.")
        return agents_data


    def define_agent(self, agent, search_tool, llm):
        role_var = agent['role'].replace(' ', '_').replace('-', '_').replace('.', '_').replace(' ', '')
        role_value = agent['role'].replace('"', '\\"').replace("'", "\\'")
        backstory = agent['backstory'].replace('"', '\\"').replace("'", "\\'")
        delegation = 'True' if agent['allow_delegation'] == 'True' else 'False'
        return (
            f'{role_var} = Agent(\n'
            f'    role="{role_value}",\n'
            f'    goal="{agent["goal"]}",\n'
            f'    backstory="{backstory}",\n'
            f'    verbose=True,\n'
            f'    allow_delegation={delegation},\n'
            f'    llm={llm},\n'  # <----- passing our llm reference here
            f'    tools=[{search_tool}]\n'
            ')\n\n'
        )

    def get_task_var_name(self, role):
        return f'task_{role.replace(" ", "_").replace("-", "_").replace(".", "_")}'

    def define_task(self, agent):
        task_var = self.get_task_var_name(agent['role'])
        task_description = agent["assigned_task"].strip().replace('"', '\\"')
        return (
            f'{task_var} = Task(\n'
            f' description="{task_description}",\n'
            f' agent={agent["role"].replace(" ", "_").replace("-", "_").replace(".", "_")},\n'
            ' verbose=True,\n'
            ')\n\n'
        )

    def generate_crew_tasks(self, agents_data):
        return ', '.join([self.get_task_var_name(agent["role"]) for agent in agents_data])

    def write_crewai_script(self, agents_data, crew_tasks, file_name):
        # Implementation to handle agents_data, crew_tasks, and file_name
        crew_agents = ', '.join([agent['role'].replace(' ', '_').replace('-', '_').replace('.', '_') for agent in agents_data])
        with open(os.path.join("scripts", file_name), 'w') as file:
            # Script header and imports
            file.write(
                'import os\n'
                'from langchain_community.llms import Ollama\n'
                'from langchain_community.tools import DuckDuckGoSearchRun\n'
                'from crewai import Agent, Task, Crew, Process\n'
                'import openai\n\n'
            )

            # Check and write LLM configuration based on settings
            if self.llm_endpoint_within_generated_scripts == 'ollama':
                if self.add_ollama_host_url_to_crewai_scripts:
                    file.write(f'ollama_host = "{self.ollama_host}"\n')  
                file.write(f'ollama = Ollama(model="{self.llm_model_within_generated_scripts}", base_url=ollama_host)\n')
            elif self.llm_endpoint_within_generated_scripts == 'openai':
                if self.add_api_keys_to_crewai_scripts:
                    file.write(f'os.environ["OPENAI_API_KEY"] = "{self.openai_api_key}"\n')
                file.write(f'llm = openai.ChatCompletion.create(model="{self.openai_model}")\n')

            # Other script content
            file.write('search_tool = DuckDuckGoSearchRun()\n\n')

            # Define agents and their tasks
            for agent in agents_data:
                agent_var = self.define_agent(agent, "search_tool", 'ollama' if self.llm_endpoint_within_generated_scripts == 'ollama' else 'llm')
                file.write(agent_var + '\n')
                task_var = self.define_task(agent)
                file.write(task_var + '\n')

            # Define crew
            crew_tasks = ', '.join([self.get_task_var_name(agent["role"]) for agent in agents_data])
            file.write(
                'crew = Crew(\n'
                f'    agents=[{crew_agents}],\n'
                f'    tasks=[{crew_tasks}],\n'
                '    verbose=True,\n'
                '    process=Process.sequential,\n'
                ')\n\n'
                'result = crew.kickoff()\n\n'
            )
        logging.info(f"Script saved at: {os.path.join('scripts', file_name)}")  # Log the full path of the saved file



    
    def call_llm_with_retry(self, instruction, overall_goal, process_response_func):
        max_attempts = 3
        for attempt in range(max_attempts):
            logging.info(f"LLM call attempt {attempt + 1} for the goal: '{overall_goal}'")
            response = self.get_agent_data(instruction, ',')  # Assuming get_agent_data makes the actual LLM call
            if not response:
                logging.error('No response from LLM')
                if attempt == max_attempts - 1:
                    raise ValueError("Failed to get valid response from LLM after 3 attempts.")
                continue

            try:
                return process_response_func(response)
            except ValueError as e:
                logging.error(f"Attempt {attempt + 1} failed: {e}")
                if attempt == max_attempts - 1:
                    raise ValueError("Failed to process LLM response after 3 attempts.")
    def generate_single_script(self, i, num_scripts, overall_goal):
    # Define a function to process LLM response
        def process_response(response):
            file_path = self.save_csv_output(response, overall_goal)
            agents_data = self.parse_csv_data(response, delimiter=',', filename=file_path)
            if not agents_data:
                raise ValueError('No agent data parsed')
            file_name = os.path.basename(file_path).replace('.csv', '.py')
            crew_tasks = self.generate_crew_tasks(agents_data)
            self.write_crewai_script(agents_data, crew_tasks, file_name)
            return file_path

        instruction = (
            f'Create a dataset in a CSV format with each field enclosed in double quotes, '
            f'for a team of agents with the goal: "{overall_goal}". '
            f'Use the delimiter "," to separate the fields. '
            'Include columns "role", "goal", "backstory", "assigned_task", "allow_delegation". '
            'Each agent\'s details should be in quotes to avoid confusion with the delimiter. '
            'Provide a single-word role, individual goal, brief backstory, assigned task, and delegation ability (True/False) for each agent.'
        )

        # Call the LLM with retry logic
        return self.call_llm_with_retry(instruction, overall_goal, process_response)

    def call_llm_with_retry(self, instruction, overall_goal, process_response_func):
        max_attempts = 3
        for attempt in range(max_attempts):
            logging.info(f"LLM call attempt {attempt + 1} for the goal: '{overall_goal}'")
            response = self.get_agent_data(instruction, ',')  # Assuming get_agent_data makes the actual LLM call
            if not response:
                logging.error('No response from LLM')
                if attempt == max_attempts - 1:
                    raise ValueError("Failed to get valid response from LLM after 3 attempts.")
                continue

            try:
                return process_response_func(response)
            except ValueError as e:
                logging.error(f"Attempt {attempt + 1} failed: {e}")
                if attempt == max_attempts - 1:
                    raise ValueError("Failed to process LLM response after 3 attempts.")


    def generate_scripts(self, overall_goal, num_scripts):
        csv_file_paths = []
        for i in range(num_scripts):
            print(f"Generating crew {i + 1} of {num_scripts} with the overall goal of '{overall_goal}'...")
            file_path = self.generate_single_script(i, num_scripts, overall_goal)
            csv_file_paths.append(file_path)
        return csv_file_paths


    def rank_crews(self, csv_file_paths, overall_goal, verbose=False):
        ranked_crews = []
        overall_summary = ""

        # Determine connection type and model for LLM
        if self.llm_endpoint == 'ollama':
            connection_type = "remote" if self.use_remote_ollama_host else "local"
            model = self.llm_model
            llm_name = f"Ollama using model {model}"
        else:
            connection_type = "remote"
            model = self.openai_engine
            llm_name = f"OpenAI using engine {model}"

        print(f"Initializing {connection_type} connection to {llm_name} for ranking crews with the overall goal of '{overall_goal}'...")

        concatenated_csv_data = 'crew_name,role,goal,backstory,assigned_task,allow_delegation\n'
        for file_path in csv_file_paths:
            try:
                with open(file_path, 'r') as file:
                    csv_data = file.read().strip()
                if csv_data.count('\n') < 1:
                    continue
                csv_data = csv_data[csv_data.index('\n') + 1:]  # Skip the first line (the remark)
                concatenated_csv_data += csv_data + '\n'
            except Exception as e:
                logging.error(f"Error processing file {file_path}: {e}")

        if concatenated_csv_data.strip() == 'crew_name,role,goal,backstory,assigned_task,allow_delegation':
            logging.warning("No valid data found in the provided CSV files.")
            return [], "No ranking could be performed due to insufficient data."

        csv_reader = csv.DictReader(io.StringIO(concatenated_csv_data))
        json_data = [row for row in csv_reader]
        json_data_str = json.dumps(json_data)
        if verbose:
            logging.debug('\nConcatenated CSV Data:')
            logging.debug(concatenated_csv_data)

        crew_names_str = ', '.join([os.path.basename(file_path).split('-')[-1].split('.')[0] for file_path in csv_file_paths])
        prompt = (
            f"Analyze the following list of crews ({crew_names_str}) to determine their suitability for successfully completing the task: "
            f"{overall_goal}. The crews are represented in a JSON object format: {json_data_str}. "
            "Please provide a ranking of the crews by their names, with the most suitable crew listed first. "
            "Also, provide a brief critique for each crew, highlighting their strengths and weaknesses."
        )

        num_tokens = self.count_tokens(prompt)
        if num_tokens > 10000:
            raise ValueError(f"The prompt is too long ({num_tokens} tokens). It must be less than 10,000 tokens.")
        if verbose:
            print(f"Number of tokens in the prompt: {num_tokens}")

        if self.llm_endpoint == 'ollama' and self.ollama:
            ranked_crew = self.ollama.invoke(prompt)
        elif self.llm_endpoint == 'openai' and self.openai_api_key:
            client = OpenAI(api_key=self.openai_api_key)
            chat_completion = client.chat.completions.create(model=self.openai_model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=self.openai_max_tokens)
            ranked_crew = chat_completion.choices[0].message.content.strip()
        else:
            logging.error("Neither OpenAI API key nor Ollama instance is available.")
            return []

        print(f"Number of tokens in the response: {self.count_tokens(ranked_crew)}")


        ranked_crews.append((concatenated_csv_data, ranked_crew))
        overall_summary += f'\n\nCrews in the following CSV files were ranked:\n'
        for file_path in csv_file_paths:
            overall_summary += f'{file_path}\n'
        overall_summary += f'\nRanking Summary:\n{ranked_crew}'

        return ranked_crews, overall_summary


    def run(self, overall_goal, num_scripts, auto_run, verbose):
        if num_scripts is None:
            num_scripts = 1  # Default value if not provided
        csv_file_paths = self.generate_scripts(overall_goal, num_scripts)
        if auto_run:
            for path in csv_file_paths:
                script_path = path.replace('.csv', '.py')  # Change the file extension to .py
                subprocess.run([sys.executable, script_path])  # Using sys.executable

    def get_existing_scripts(self, overall_goal):
        # Assuming scripts are stored in a directory named "scripts"
        script_dir = os.path.join(os.getcwd(), "scripts")
        return [os.path.join(script_dir, f) for f in os.listdir(script_dir) if f.endswith('.csv') and overall_goal[:40] in f]

    def save_ranking_output(self, ranked_crews, overall_goal):
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        file_name = f'crewai-autocrew-{timestamp}-{overall_goal[:40].replace(" ", "-")}-ranking.csv'
        directory = os.path.join(os.getcwd(), "scripts")
        if not os.path.exists(directory):
            os.makedirs(directory)
        file_path = os.path.join(directory, file_name)
        with open(file_path, 'w') as file:
            writer = csv.writer(file)
            writer.writerow(["crew_name", "ranking"])
            for crew_name, ranking in ranked_crews:
                writer.writerow([crew_name, ranking])
        logging.info(f"Ranking CSV saved at: {file_path}")  # Log the full path of the saved ranking CSV

    def log_config_with_redacted_api_keys(self):
        redacted_config = configparser.ConfigParser()
        for section in self.config.sections():
            redacted_config.add_section(section)
            for key, value in self.config.items(section):
                if section == 'AUTHENTICATORS':
                    # Redact all values in the AUTHENTICATORS section except the last 4 characters
                    redacted_value = '*' * (len(value) - 4) + value[-4:]
                else:
                    redacted_value = value
                redacted_config.set(section, key, redacted_value)
        
        config_string = io.StringIO()
        redacted_config.write(config_string)
        logging.info("Redacted config.ini content:\n" + config_string.getvalue())



    @staticmethod
    def redact_api_key(api_key):
        # Redact all but the last 4 characters of the API key
        if len(api_key) > 4:
            return '*' * (len(api_key) - 4) + api_key[-4:]
        else:
            return api_key  # If the key is too short, just return it as is


               
    

def main():
    # Global exception handler
    def handle_exception(exc_type, exc_value, exc_traceback):
        if issubclass(exc_type, KeyboardInterrupt):
            # Call the default KeyboardInterrupt handler
            sys.__excepthook__(exc_type, exc_value, exc_traceback)
            return
        logging.error("Uncaught exception", exc_info=(exc_type, exc_value, exc_traceback))

    # Set the global exception handler
    sys.excepthook = handle_exception

    # Parse command line arguments for verbosity first
    parser = argparse.ArgumentParser(description='CrewAI Autocrew Script')
    parser.add_argument('-v', '--verbose', action='store_true', help='Provide additional details during execution')
    args, remaining_argv = parser.parse_known_args()

    # Initialize logging with the appropriate verbosity level
    initialize_logging(verbose=args.verbose)

    # Define the message to be displayed and logged
    startup_message = (
        f"\nAutocrew version: {AUTOCREW_VERSION}\n"
        "\n"
        "Settings can be modified within \"config.ini\"\n"
        "Generated scripts are saved in the \"scripts\" subdirectory\n"
        "If you experience any errors, please create an issue on Github and attach \"autocrew.log\":\n"
        "https://github.com/yanniedog/autocrew/issues/new\n"
        "\n"
    )

    # Log the startup message
    logging.info(startup_message)

    # Continue parsing the remaining command line arguments
    parser.add_argument('overall_goal', nargs='?', type=str, help='The overall goal for the crew')
    parser.add_argument('-m', '--multiple', type=int, nargs='?', const=-1, default=None, metavar='NUM', help='Create NUM number of CrewAI scripts for the same overall goal. Example: -m 3')
    parser.add_argument('-r', '--rank', action='store_true', help='Rank the generated crews if multiple scripts are created')
    parser.add_argument('-a', '--auto_run', action='store_true', help='Automatically run the scripts after generation')
    args = parser.parse_args(remaining_argv)

    if args.overall_goal is None:
        args.overall_goal = input("Please set the overall goal for your crew: ")

    if args.multiple == -1:
        while True:
            try:
                args.multiple = int(input("Please specify the total number of alternative scripts to generate: "))
                if args.multiple > 0:
                    break
                else:
                    logging.info("Please enter a positive integer.")
            except ValueError:
                logging.error("Invalid input. Please enter a valid number.")

    autocrew = AutoCrew()

    # Log the redacted config.ini content
    autocrew.log_config_with_redacted_api_keys()

    try:
        if args.multiple:
            csv_file_paths = autocrew.generate_scripts(args.overall_goal, args.multiple)
            if args.rank:
                ranked_crews, overall_summary = autocrew.rank_crews(csv_file_paths, args.overall_goal, args.verbose)
                logging.info(f"\nRanking prompt:\n{overall_summary}\n")
                autocrew.save_ranking_output(ranked_crews, args.overall_goal)
            if args.auto_run:
                for path in csv_file_paths:
                    script_path = path.replace('.csv', '.py')  # Change the file extension to .py
                    subprocess.run([sys.executable, script_path])  # Using sys.executable
        elif args.rank:
            csv_file_paths = autocrew.get_existing_scripts(args.overall_goal)
            ranked_crews, overall_summary = autocrew.rank_crews(csv_file_paths, args.overall_goal, args.verbose)
            logging.info(f"\nRanking prompt:\n{overall_summary}\n")
            autocrew.save_ranking_output(ranked_crews, args.overall_goal)
            if args.auto_run:
                subprocess.run([sys.executable, ranked_crews[0][0]])
        else:
            autocrew.run(args.overall_goal, None, args.auto_run, args.verbose)
    except Exception as e:
        logging.exception("An error occurred during script execution.")
        sys.exit(1)

if __name__ == '__main__':
    main()
