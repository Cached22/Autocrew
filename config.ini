[BASIC]
# specify either ollama or openai as the LLM endpoint
llm_endpoint = ollama

[OLLAMA_CONFIG]
llm_model = openhermes

[OPENAI_CONFIG]
openai_model = gpt-3.5-turbo
# Specify the maximum token limit (input + output) for your chosen LLM model:
max_tokens = 4096

[CREWAI_SCRIPTS]
llm_endpoint_within_generated_scripts = ollama
llm_model_within_generated_scripts = openhermes
add_api_keys_to_crewai_scripts = true
add_ollama_host_url_to_crewai_scripts = true
overall_goal_truncation_for_filenames = 40

[AUTHENTICATORS]
openai_api_key = 
ngrok_auth_token = 
ngrok_api_key = 

[REMOTE_HOST_CONFIG]
reset_ollama_host_on_startup = false
use_remote_ollama_host = false
name_of_remote_ollama_host = ngrok

[MISCELLANEOUS]
# on_screen_loggin_level can be DEBUG, INFO, WARNING, ERROR, CRITICAL.
# However, a complete debug log is also saved to autocrew.log, and is overwritten each time the program is run. (Default = INFO)
on_screen_logging_level = INFO
